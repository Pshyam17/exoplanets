# -*- coding: utf-8 -*-
"""3. ML Pipeline (Kepler & TESS Data)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/yashvardhangupta37/3-ml-pipeline-kepler-tess-data.0d5dd540-a17f-49b4-b0b5-bf536860437f.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251002/auto/storage/goog4_request%26X-Goog-Date%3D20251002T001801Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D232c4867aa508f3a011e5f5972100cddcf3d8cb4e230e5eed23f5b3307f0c2d283159340dbf23c8e66ce3c281c6160e05a8785e0d259a240c0ec33629db57cbba4b06b2a30c3429cb5e9a5ba02980c9f823c0f12c96e2a052daec5b12b69a6e062e871bba53fa9702e085e98a6ed4dc50ce80230c2356949a645032110165ab45c69379611ee5914669833f98c1d4b6202adefc4ae6a0b5ee2b6d19b3d8183f7fd60c8164bf4a4cdfccb019d60eb3f0f762b3aaee423089e2c5d00fa43db2a766e1b32e955f9224e758fac63ec9442e0746017e1bc8fca69cbb54a69e88ebe05bbb5ca85572620da3c5b45368f2a0d118a38d70d33f2200a46a3cae939da172a
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# Install necessary packages
!pip install lightkurve --quiet

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier  # Or another classifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the dataset
koi_table = pd.read_csv("/kaggle/input/kepler-and-tess-exoplanet-data/q1_q8_koi_2025.02.03_04.12.15.csv", skiprows=1, delimiter=",", comment="#")

# 2. Feature Engineering (Insolation Flux) - Add this if you want to use it
koi_table['koi_insol'] = koi_table['koi_steff']**4 / koi_table['koi_period']**2  # Simplified formula


# 3. Define Features (including insolation flux)
features = ["koi_period", "koi_prad", "koi_teq", "koi_srad", "koi_slogg", "koi_steff", "koi_insol"] # Include koi_steff, koi_insol
# features = ["koi_period", "koi_prad", "koi_teq", "koi_srad", "koi_slogg", "koi_steff"] # If you don't want koi_insol

# 4. Map labels
koi_table["label"] = koi_table["koi_disposition"].map({"CONFIRMED": 1, "FALSE POSITIVE": 0, "CANDIDATE":1}) # Include CANDIDATE as 1

# 5. Impute missing values (using SimpleImputer)
imputer = SimpleImputer(strategy='mean')  # Or 'median'
koi_table[features] = imputer.fit_transform(koi_table[features])

# 6. Drop rows with missing labels (if any)
koi_table = koi_table.dropna(subset=["label"])

# 7. Prepare data for ML
X = koi_table[features]
y = koi_table["label"]

# 8. Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 9. Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# 10. Choose and train a model (RandomForestClassifier example)
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}
model = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(model, param_grid, cv=3, scoring='roc_auc', n_jobs = -1) # Use all available cores
grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_


# 11. Evaluate the model
y_pred = best_model.predict(X_test)
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

auc_roc = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])
print(f"AUC-ROC: {auc_roc}")

# 12. Feature importances (optional)
feature_importances = best_model.feature_importances_
print("Feature Importances:")
for i, feature in enumerate(features):
    print(f"{feature}: {feature_importances[i]}")

koi_table.columns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Split into training (80%) and testing (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Normalize data (important for ML models)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train a Random Forest Model
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train_scaled, y_train)

# Predict on the test set
y_pred = clf.predict(X_test_scaled)

# Print accuracy score
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")

# Print classification report
print(classification_report(y_test, y_pred))

# Plot Confusion Matrix
plt.figure(figsize=(6,5))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues", xticklabels=["False Positive", "Confirmed"], yticklabels=["False Positive", "Confirmed"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# Extract feature importance scores
importances = clf.feature_importances_

# Plot feature importance
plt.figure(figsize=(8,5))
sns.barplot(x=importances, y=features)
plt.xlabel("Feature Importance Score")
plt.ylabel("Features")
plt.title("Which Features Are Most Important?")
plt.show()


plt.figure(figsize=(10, 6))  # Adjust figure size for better readability
sns.barplot(x=importances, y=features, orient='h')  # Horizontal bar plot
plt.xlabel("Feature Importance Score")
plt.ylabel("Features")
plt.title("Feature Importances (Descending)")
plt.gca().invert_yaxis()  # Invert y-axis to show most important features at the top
plt.tight_layout()  # Adjust layout to prevent labels from overlapping
plt.show()

toi_table = pd.read_csv("/kaggle/input/kepler-and-tess-exoplanet-data/TOI_2025.02.03_06.18.31.csv",
                         skiprows=1, delimiter=",", comment="#")

toi_table.columns

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier  # Or your chosen model
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load koi_table and train your model (as before - ensure 'features' is defined)
# ... (Your existing code for loading koi_table, feature engineering, etc.)

# 2. Load TOI table
# toi_table = pd.read_csv("your_toi_table.csv")  # Replace with your TOI file path

# 3. Define Feature Mapping (same as before)
feature_mapping = {
    "koi_period": "pl_orbper",
    "koi_prad": "pl_rade",
    "koi_teq": "pl_eqt",
    "koi_srad": "st_rad",
    "koi_slogg": "st_logg",
    "koi_steff": "st_teff",
    "koi_insol": "pl_insol"
}

toi_features = []
for koi_feature, toi_feature in feature_mapping.items():
    if toi_feature in toi_table.columns:
        toi_features.append(toi_feature)
    else:
        print(f"Warning: TOI table does not have a column matching '{koi_feature}'. Skipping.")

print("Using TOI features:", toi_features)

# 4. Prepare new data (using the mapped names)
new_data = toi_table[toi_features].copy()

# 5. Rename columns in new_data to match the training data features
reverse_mapping = {v: k for k, v in feature_mapping.items()}  # Create reverse mapping
new_data = new_data.rename(columns=reverse_mapping)  # Rename columns

# 6. Impute missing values (if any)
imputer_toi = SimpleImputer(strategy='mean')  # Or keep the same imputer if you prefer.
new_data[features] = imputer_toi.fit_transform(new_data[features])

# 7. Scale the new data (using the SAME scaler)
new_data_scaled = scaler.transform(new_data)  # Now the feature names should match!

# 8. Predict using the trained model
predictions = best_model.predict(new_data_scaled)

# 9. Add predictions to the TOI table
toi_table["Predicted Label"] = predictions

# 10. Display results (adapt column names as needed)
print(toi_table[toi_table["Predicted Label"] == 1][["toi", "pl_rade", "pl_orbper"]])  # Adapt column names

""""What do the column values in the output actually tell us about the results of our code?"

**The Output Table:**

The `toi_table`, filtered to show only the TOIs that model predicted as likely confirmed exoplanets.

* **`toi`:** This is the TESS Object of Interest ID. It's a unique identifier for each potential exoplanet candidate observed by the TESS mission.

* **`pl_rade`:** This is the predicted or measured radius of the planet, expressed in Earth radii.  A larger value indicates a larger planet.  Remember that some of these values might be `NaN` if the radius was not available in the original TESS data.

* **`pl_orbper`:** This is the orbital period of the planet, expressed in days. It tells you how long it takes for the planet to complete one orbit around its star. A shorter period generally means the planet is closer to its star.

**What the Values Indicate:**

* **TOI ID:** The `toi` column helps identify and look up more information about specific TOI candidates in the TESS catalog or other exoplanet databases.

* **Planet Radius (`pl_rade`):** The values in this column helps understand the size distribution of the planets your model predicts to be confirmed. We can compare these radii to the radii of known exoplanets or analyze them statistically.

* **Orbital Period (`pl_orbper`):** The orbital period values give insights into the orbital characteristics of the predicted planets. We can study the distribution of orbital periods, look for correlations between period and other planet properties, and compare the periods to those of known exoplanets.

**Interpreting the Results:**

1. **Confirmation:** The rows in the table means model has identified some TOIs as likely exoplanets.

2. **Planet Size and Orbit:** The `pl_rade` and `pl_orbper` values provide physical characteristics of the predicted planets, allowing us to compare them with known exoplanets and understand their properties.

3. **Model Performance:** To assess how well the model is doing, we need to evaluate its performance using metrics like accuracy, precision, recall, F1-score, and AUC-ROC.  These metrics compare the model's predictions to the actual classifications of the TOIs.

   

**In summary:** The table shows the TOIs that the model predicts are likely confirmed exoplanets, along with their radius and orbital period.

"""

# Taking 'koi_period' and 'koi_prad' are important features
plt.figure(figsize=(8, 6))
sns.scatterplot(x='koi_period', y='koi_prad', hue='label', data=koi_table)
plt.xlabel("Orbital Period")
plt.ylabel("Planetary Radius")
plt.title("Scatter Plot of Period vs. Radius")
plt.show()

correlation_matrix = koi_table[features].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix of Features")
plt.show()